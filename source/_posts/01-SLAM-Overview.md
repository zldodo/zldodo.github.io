---
title: Have Fun with SLAM-01-Overview
date: 2017-10-10 21:14:05
tags: 
    - SLAM
    - Cartographer
categories:
    - 探知 

---
认知环境，并能自我定位，是自主式机器人在真实环境中执行任务时非常重要的一项能力。随着无人技术和智能产品不断涌现，SLAM(Simultaneous Localization and Mapping)，即时定位与地图构建，作为其中的一种基础技术，受到了学业界和工业界的高度关注。

在自身兴趣和课业要求的双从驱动下，我从今年开始学习SLAM的相关知识。在这期间，领略到了SLAM领域的博大精深，数学基础要求比较高，特别是概率学和优化。当还沉浸在漫天的公式无法自拔之时，你会发现，这些理论不过是SLAM海洋的冰山一角。因为实际场景往往有实时性要求，所以，coding 才是更大的挑战，而我还没开始真正踏足这一部分。

## 什么是SLAM?
SLAM问题可以描述为: 机器人在未知环境中从一个未知位置开始移动，在移动过程中根据位置估计和地图进行自身定位，同时在自身定位的基础上建造增量式地图，实现机器人的自主定位和导航。

有些费解是不是？相信听过接下来的故事后，你就会对SLAM有一些intuitions了。

>从前，有一个机器人叫“小萝卜”。它长着一双乌黑发亮的大眼睛，叫做Kinect（也可以是其他传感器）。有一天，它被邪恶的科学家关进了一间空屋子，里面放满了杂七杂八的东西。小萝卜感到很害怕，因为这个地方他从来没来过，一点儿也不了解。让他感到害怕的主要是三个问题：
　　1. 自己在哪里？
　　2. 这是什么地方？
　　3. 怎么离开这个地方？
在SLAM理论中，第一个问题称为定位 (Localization)，第二个称为建图 (Mapping)，第三个则是随后的路径规划。

这个故事出自高翔博士，有兴趣的同学可以去膜拜下他的博客哦~[链接](http://www.cnblogs.com/gaoxiang12/p/3695962.html)

## 如何进行SLAM？
**首先，“眼睛是心灵的窗户”。**机器人要认识这个大千世界，需要给它配备用来感知环境的传感器，就像人类或其他生物拥有眼睛一样。目前，通常有两类传感器：
- 激光雷达，直接获得相对于环境的直接距离信息，从而实现直接相对定位。
- 视觉传感器，通过两帧或多帧图像来估计自身的位姿变化，再通过累积位姿变化计算当前位置。

**然后，根据传感器类型，选择匹配的SLAM方案。**主流的SLAM方案如下：
- 激光雷达
Cartographer,HectorSLAM, Gmapping 
- 视觉传感器
根据计算数据量的大小，可以分为稀疏法(特征点)、半稠密法和稠密法
。根据摄像头的数目和特性，又可分为单目，双目，RGBD。常见的视觉SLAM算法有ORB-SLAM，LSD-SLAM，DTAM。关于这部分，有兴趣的同学可以详见[泡泡机器人wiki](http://www.slamcn.org/index.php/%E9%A6%96%E9%A1%B5)。

## SLAM系统框架
以HectorSLAM为例，其系统框架如下图所示。HectorSLAM 算法需要高更新频率、小测量噪声的激光扫描仪，但导航子系统不是必须的，使空中无人机与地面小车在不平坦区域运行存在运用的可能性。

![SLAM系统框架](01-SLAM-Overview/系统框图.jpg)

## SLAM算法框架
SLAM算法的核心分为三个步骤：
1. 预处理。例如，对激光雷达原始数据所优化，剔除一些有问题的数据，或者进行滤波。
2. 匹配。也就是说把当前这一个局部环境的点云数据在已经建立地图上寻找到对应的位置。
3. 地图融合。将这一轮来自激光雷达的新数据拼接到原始地图当中，最终完成地图的更新。

细化来说，SLAM主要包含以下模块：
- sensor data process
- Visual Odometry，前端优化/定位，针对视觉SLAM
- Backend(Optimization),后端/全局优化,例如GraphSLAM，Cartographer里的SPA
- Mapping, 常见有Matrix and Topologic map两种形式，各有优缺
- Loop cloure detection,回环检测, 进一步减少累计误差

这篇文章仅仅涉及了SLAM最最最基本的概念，需要学习的还有很多很多。希望有兴趣玩耍SLAM的同学，我们可以一起相互交流学习哟~


